{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Case study 4: Extracting a polygon and aggregate data split by a polygon mask\n",
    "###\n",
    "\n",
    "#### Miguel D. Mahecha, Fabian Gans et al. (correspondence to: mmahecha@bgc-jena.mpg.de and fgans@bgc-jena.mpg.de)\n",
    "\n",
    "* Notebook to reproduce and understand examples in the paper *Earth system data cubes unravel global multivariate dynamics* (sub.).\n",
    "\n",
    "* The NB is written based on Julia 1.3\n",
    "\n",
    "* Normal text are explanations referring to notation and equations in the paper\n",
    "\n",
    "* `# comments in the code are intended explain specific aspects of the coding`\n",
    "\n",
    "* ### New steps in workflows are introduced with bold headers\n",
    "\n",
    "Sept 2019, Max Planck Institute for Biogeochemistry, Jena, Germany"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# for operating the Earth system data lab\n",
    "using ESDL, WeightedOnlineStats"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Select and subet an Earth system data cube\n",
    "\n",
    "We need to choose a cube and here select a 8-dayily, 0.25Â° resolution global cube. The cube name suggests it is chunked such that we have one time chunk and 720x1440 spatial chunks"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cube_handle = Cube(\"../data/subcube\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Here we define two subcubes for Gross primary productivity and for surface moisture"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gpp = subsetcube(cube_handle, variable = \"gross_primary_productivity\", time = 2003:2012)\n",
    "moisture = subsetcube(cube_handle, variable = \"surface_moisture\", time = 2003:2012)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The objective is to estimate histograms of gross_primary_productivity and surface moisture and split them by AR5 region. We first download a shapefile defining these regions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cd(\"../data\") do\n",
    "    if !isfile(\"referenceRegions.shp\")\n",
    "        p = download(\"https://www.ipcc-data.org/documents/ar5/regions/referenceRegions.zip\")\n",
    "        run(`unzip $p`)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "After this we can use the shapefile and apply a rasterization method to convert it to a cube. The `labelsym` argument specifies which field to transfer to the cubes metadata."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "srex = cubefromshape(\"../data/referenceRegions.shp\",gpp,labelsym=:LAB)\n",
    "using ESDLPlots\n",
    "plotMAP(srex)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In order to compute some aggregate statistics over our datasets we join the 3 data cubes into a single iterable table. The data is not loaded but can be iterated over in an efficient manner which is chunk-aware. Additionally we need the latitude values of the Table to compute the weights of our aggregation which represent the grid cell size."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "t = CubeTable(gpp = gpp, moisture=moisture, region=srex, include_axes=(\"lat\",))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "If the concept of this table is still a bit opaque, we demonstrate this by converting a small part of the table to a DataFrame. We just apply a filter to sort out missings and then take 10 values of the Table."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using DataFrames, Base.Iterators\n",
    "DataFrame(take(Iterators.filter(r->!any(ismissing,(r.gpp,r.moisture,r.region)),t),10))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now comes the actual aggregation. First we generate an empty `WeightedHist` for every SREX region. Then we loop through all the entries in our table and fit the gpp/moisture pair into the respective histogram. In the end we create a new (in-memory) data cube from the resulting histograms."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ProgressMeter\n",
    "function aggregate_by_mask(t,labels)\n",
    "    hists = [WeightedHist((0.0:1:12,0:0.1:1)) for i=1:33]\n",
    "    @showprogress for row in t\n",
    "        if !any(ismissing,(row.gpp, row.moisture, row.region))\n",
    "            h = hists[row.region[]]\n",
    "            fit!(h,(row.gpp,row.moisture),cosd(row.lat))\n",
    "        end\n",
    "    end\n",
    "    ##We create the axes for the new output data cube\n",
    "    midpointsgpp   = 0.5:1.0:11.5\n",
    "    midpointsmoist = 0.05:0.1:0.95\n",
    "    newaxes = CubeAxis[\n",
    "        CategoricalAxis(\"SREX\",[labels[i] for i in 1:33]),\n",
    "        RangeAxis(\"GPP\",midpointsgpp),\n",
    "        RangeAxis(\"Moisture\",midpointsmoist),\n",
    "    ]\n",
    "    # And create the new cube object\n",
    "    data = [WeightedOnlineStats.pdf(hists[reg],(g,m)) for reg in 1:33, g in midpointsgpp, m in midpointsmoist]\n",
    "    CubeMem(newaxes,data)\n",
    "end\n",
    "r = aggregate_by_mask(t,srex.properties[\"labels\"])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "And we create some heatmap plots:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for reg in (\"AMZ\",\"CEU\",\"EAF\")\n",
    "    data = r[srex=reg][:,:]\n",
    "    p = heatmap(0.5:1.0:11.5,0.05:0.1:0.95,(data')./maximum(data), lw = 0, xlab = \"gross primary productivity\", ylab=\"surface moisture\", title=reg)\n",
    "    Plots.savefig(p,\"../figures/heatmap_$reg.png\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0-rc5.1"
  },
  "kernelspec": {
   "name": "julia-1.3",
   "display_name": "Julia 1.3.0-rc5.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
