{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Case study 4: Extracting a polygon and aggregate data split by a polygon mask\n",
    "###\n",
    "\n",
    "#### Miguel D. Mahecha, Fabian Gans et al. (correspondence to: mmahecha@bgc-jena.mpg.de and fgans@bgc-jena.mpg.de)\n",
    "\n",
    "* Notebook to reproduce and understand examples in the paper *Earth system data cubes unravel global multivariate dynamics* (sub.).\n",
    "\n",
    "* The NB is written based on Julia 1.3\n",
    "\n",
    "* Normal text are explanations referring to notation and equations in the paper\n",
    "\n",
    "* `# comments in the code are intended explain specific aspects of the coding`\n",
    "\n",
    "* ### New steps in workflows are introduced with bold headers\n",
    "\n",
    "Sept 2019, Max Planck Institute for Biogeochemistry, Jena, Germany"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# for operating the Earth system data lab\n",
    "using ESDL, WeightedOnlineStats"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Load the pre-downloaded Earth-System Datacube"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cube_handle = Cube(\"../data/subcube\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Here we define two subcubes for Gross primary productivity and for surface moisture"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gpp = subsetcube(cube_handle, variable = \"gross_primary_productivity\", time = 2003:2012)\n",
    "moisture = subsetcube(cube_handle, variable = \"surface_moisture\", time = 2003:2012)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The objective is to estimate histograms of gross_primary_productivity and surface moisture and split them by AR5 region. We first download a shapefile defining these regions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cd(\"../data\") do\n",
    "    if !isfile(\"referenceRegions.shp\")\n",
    "        p = download(\"https://www.ipcc-data.org/documents/ar5/regions/referenceRegions.zip\")\n",
    "        run(`unzip $p`)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "After this we can use the shapefile and apply a rasterization method to convert it to a cube. The `labelsym` argument specifies which field to transfer to the cubes metadata."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "srex = cubefromshape(\"../data/referenceRegions.shp\",gpp,labelsym=:LAB)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In order to compute some aggregate statistics over our datasets we join the 3 data cubes into a single iterable table. The data is not loaded but can be iterated over in an efficient manner which is chunk-aware. Additionally we need the latitude values of the Table to compute the weights of our aggregation which represent the grid cell size."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "t = CubeTable(gpp = gpp, moisture=moisture, region=srex, include_axes=(\"lat\",))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "If the concept of this table is still a bit opaque, we demonstrate this by converting a small part of the table to a DataFrame. We just apply a filter to sort out missings and then take 10 values of the Table."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using DataFrames, Base.Iterators\n",
    "DataFrame(take(Iterators.filter(r->!any(ismissing,(r.gpp,r.moisture,r.region)),t),10))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now comes the actual aggregation. First we generate an empty `WeightedHist` for every SREX region. Then we loop through all the entries in our table and fit the gpp/moisture pair into the respective histogram. Never will the whole cube be loaded into memory, but only one chunk is read at a time. In the end we create a new (in-memory) data cube from the resulting histograms."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ProgressMeter\n",
    "function aggregate_by_mask(t,labels)\n",
    "    n_classes = length(labels)\n",
    "    # Here we create an empty 2d histogram for every SREX region\n",
    "\n",
    "    ####hists = [WeightedHist((0.0:1:12,0:0.1:1)) for i=1:n_labels]\n",
    "    hists = [WeightedHist((0.0:0.1:12,0:0.01:1)) for i=1:n_classes]\n",
    "\n",
    "    # Now loop through every data point (in space and time)\n",
    "    @showprogress for row in t\n",
    "        # If all data are there\n",
    "        if !any(ismissing,(row.gpp, row.moisture, row.region))\n",
    "            ####We select the appropriate histogram according to the region the data point belongs to\n",
    "            h = hists[row.region[]]\n",
    "            ####And we fit the two data points to the histogram, weight by cos of lat\n",
    "            fit!(h,(row.gpp,row.moisture),cosd(row.lat))\n",
    "        end\n",
    "    end\n",
    "    ########We create the axes for the new output data cube\n",
    "    midpointsgpp   = 0.05:0.1:11.95\n",
    "    midpointsmoist = 0.005:0.01:0.995\n",
    "    newaxes = CubeAxis[\n",
    "        CategoricalAxis(\"SREX\",[labels[i] for i in 1:33]),\n",
    "        RangeAxis(\"GPP\",midpointsgpp),\n",
    "        RangeAxis(\"Moisture\",midpointsmoist),\n",
    "    ]\n",
    "    # And create the new cube object\n",
    "    data = [WeightedOnlineStats.pdf(hists[reg],(g,m)) for reg in 1:33, g in midpointsgpp, m in midpointsmoist]\n",
    "    CubeMem(newaxes,data)\n",
    "end\n",
    "r = aggregate_by_mask(t,srex.properties[\"labels\"])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "To illustrate the output we plot the density for the region \"Eastern Africa\":"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Plots\n",
    "Plots.heatmap(0.005:0.01:0.995,0.05:0.1:11.95,r[srex=\"EAF\"][:,:], clim=(0,5e-7), xlabel=\"Moisture\", ylabel=\"GPP\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "saveCube(r, \"../data/srex_aggregate.zarr\", overwrite = true)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Python plotting\n",
    "\n",
    "To generate the publication-quality plots we use python plotting tools with the following code, which does not demonstrate any ESDL capabilities but is included here for reproducbility:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# for plotting\n",
    "using PyCall, PyPlot, PlotUtils"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lat = getAxis(\"lat\", srex).values\n",
    "lon = getAxis(\"lon\", srex).values\n",
    "\n",
    "cm = ColorMap(get_cmap(\"gray\", 33))\n",
    "ccrs = pyimport_conda(\"cartopy.crs\",\"cartopy\")\n",
    "feat = pyimport_conda(\"cartopy.feature\",\"cartopy\")\n",
    "\n",
    "######## make new figure\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "\n",
    "ax = subplot(313, )\n",
    "\n",
    "\n",
    "######## set the projection\n",
    "ax = plt.subplot(projection=ccrs.Robinson())\n",
    "\n",
    "######## add title\n",
    "plt.title(\"IPCC AR5 regions\", fontsize=20)\n",
    "\n",
    "######## land and ocean backgrounds\n",
    "ax.add_feature(feat.LAND,  color = [0.9, 0.9, 0.9])\n",
    "ax.add_feature(feat.OCEAN, color = [0.85, 0.85, 0.85])\n",
    "ax.coastlines(resolution = \"50m\", color = [0, 0, 0], lw = 0.5)\n",
    "\n",
    "######## show data\n",
    "DAT = srex[:, :]\n",
    "DAT = replace(DAT, missing=>NaN)\n",
    "DAT2 = reverse(DAT', dims = 1)\n",
    "\n",
    "im = ax.imshow(DAT2, transform = ccrs.PlateCarree(), cmap = cm, vmin = 0, vmax = 30)\n",
    "\n",
    "for i in 1:33\n",
    "    if  !(srex.properties[\"labels\"][i] in [\"NTP*\", \"ETP*\", \"STP*\"])\n",
    "        idx_lat = mean(lat[j.I[2]] for j in CartesianIndices(DAT) if DAT[j]==i)\n",
    "        idx_lon = mean(lon[j.I[1]] for j in CartesianIndices(DAT) if DAT[j]==i)\n",
    "        ax.text(idx_lon, idx_lat, weight=\"bold\", color = \"red\", srex.properties[\"labels\"][i], horizontalalignment = \"center\", verticalalignment = \"center\", transform=ccrs.Geodetic())\n",
    "    end\n",
    "end\n",
    "\n",
    "####savefig(\"SREX.pdf\", bbox_inches = \"tight\")\n",
    "savefig(\"../figures/SREX.png\", bbox_inches = \"tight\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "And we create some heatmap plots:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gpp = collect(getAxis(\"GPP\", r).values)\n",
    "moi = collect(getAxis(\"Moisture\", r).values);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Draw a heatmap with the numeric values in each cell\n",
    "####f, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.set_style(\"white\")\n",
    "sns.axes_style(\"darkgrid\")\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "for i in 1:33\n",
    "    reg = srex.properties[\"labels\"][i]\n",
    "    figure(figsize = (5, 5))\n",
    "    data = r[srex=reg][:,:]\n",
    "    data = sqrt.(replace((data')./maximum(data),0.0=>NaN))\n",
    "\n",
    "    data = transpose(data)\n",
    "\n",
    "    ax = sns.heatmap(data,  cmap = sns.cm.rocket_r)#, vmin = 0.1, vmax=1)\n",
    "    ax.set_yticks(1:10:120)\n",
    "    ax.set_yticklabels(round.(gpp[1:10:120]; digits = 0))\n",
    "\n",
    "    ax.set_xticks(1:10:100)\n",
    "    ax.set_xticklabels(round.(moi[1:10:100]; digits = 1))\n",
    "    #ax.set_xlabel(\"Surface Moisture []\")\n",
    "    #ax.set_ylabel(\"Gross Primary Production [g C m⁻² d⁻¹]\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(reg, fontsize = 20)\n",
    "    savefig(\"normal\" * reg * \".png\", bbox_inches = \"tight\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  },
  "kernelspec": {
   "name": "julia-1.3",
   "display_name": "Julia 1.3.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
