{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Case study 2: Intrinsic dimensions of ecosystem dynamics\n",
    "### As estimate based on PCAs\n",
    "\n",
    "#### Miguel D. Mahecha, Fabian Gans et al. (correspondence to: mmahecha@bgc-jena.mpg.de and fgans@bgc-jena.mpg.de)\n",
    "\n",
    "* Notebook to reproduce and understand examples in the paper *Earth system data cubes unravel global multivariate dynamics* (sub.).\n",
    "\n",
    "* The NB is written based on Julia 1.1\n",
    "\n",
    "* Normal text are explanations referring to notation and equations in the paper\n",
    "\n",
    "* `# comments in the code are itended explain specific aspects of the coding`\n",
    "\n",
    "* ### New steps in workflows are introduced with bold headers\n",
    "\n",
    "Sept 2019, Max Planck Institute for Biogeochemistry, Jena, Germany"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Load required packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# for plotting later on (need to be loaded first, to avoid conflicts)\n",
    "using PyCall, PyPlot\n",
    "\n",
    "# for operating the Earth system data lab\n",
    "using ESDL\n",
    "\n",
    "# for parallel computing\n",
    "using Distributed"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In this study we investigate the redundancy the different variables in each pixel. Therefore we calculate a linear dimensionality reduction (PCA) and check how many dimensions are needed to explain 90% of the variance of a cube that contained originally 6 variables.  First we check out the variables from the cube and add some processors, because we want to do a global study"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Select and prepare (subset/gapfill) an Earth system data cube\n",
    "\n",
    "We need to choose a cube and here select a 8-dayly, 0.25Â° resolution global cube. The cube name suggests it is chunked such that we have one time chunk and 720x1440 spatial chunks"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cd(@__DIR__)\n",
    "cube_handle = Cube(\"../data/subcube\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "As we see here, we have a data cube of the form (compare Table 1 in the paper):\n",
    "\n",
    "$$\n",
    "    \\mathcal{C}(\\{lat, lon, time, var\\})\n",
    "$$\n",
    "\n",
    "There is a command that returns the metadata fro the variable axis for better orientation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cubeinfo(cube_handle)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# if we want the names of the variables:\n",
    "println(getAxis(\"Var\", cube_handle).values)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Having the variable names allows us to make a selection, such that we can subset the global cube. We should also take care that the variables are as complete as possible in the time window we analyze. This has been explored a priori."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# vector of variables we will work with\n",
    "vars = [\"evaporative_stress\",\n",
    "    \"latent_energy\",\n",
    "    \"black_sky_albedo_avhrr\",\n",
    "    \"fapar_tip\",\n",
    "    \"root_moisture\",\n",
    "    \"transpiration\",\n",
    "    \"white_sky_albedo_avhrr\",\n",
    "    \"sensible_heat\",\n",
    "    \"bare_soil_evaporation\",\n",
    "    \"net_radiation\",\n",
    "    \"net_ecosystem_exchange\",\n",
    "    \"evaporation\",\n",
    "    \"terrestrial_ecosystem_respiration\",\n",
    "    \"land_surface_temperature\",\n",
    "    \"leaf_area_index\",\n",
    "    \"white_sky_albedo\",\n",
    "    \"gross_primary_productivity\",\n",
    "    \"black_sky_albedo\"];\n",
    "\n",
    "# time window where most of them are complete\n",
    "timespan = Date(\"2003-01-01\")..Date(\"2011-12-31\")\n",
    "\n",
    "# subset the grand cube and get the cube we will analyse here\n",
    "cube_subset = subsetcube(cube_handle, time = timespan, variable = vars)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "An important preprocessing step is gapfilling. We do not want to enter the debate on the optimal gapfilling method. What we do here is gapfilling first with the mean seasonal cycle (where it can be estimated), and interpolating long-recurrent gaps (typically in winter seasons)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# gapfilling this requires a bit of CPU -> add some parallel processors:\n",
    "addprocs(4)\n",
    "\n",
    "# use the ESDL buit-in function\n",
    "cube_fill = gapFillMSC(cube_subset)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The interpolation of wintergpas needs a function that we code here an call `LinInterp`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Function LinInterp should be available on every core, i.e. @everywhere\n",
    "@everywhere begin\n",
    "\n",
    "    # package on each core\n",
    "    using Interpolations\n",
    "\n",
    "    function LinInterp(y)\n",
    "        try\n",
    "            # find the values we need to input\n",
    "            idx_nan = findall(ismissing, y)\n",
    "            idx_ok  = findall(!ismissing, y)\n",
    "\n",
    "            # make sure to have a homogenous input array\n",
    "            y2 = Float32[y[i] for i in idx_ok]\n",
    "\n",
    "            # generate an interpolation object based on the good data\n",
    "            itp = interpolate((idx_ok,), y2, Gridded(Linear()))\n",
    "\n",
    "            # fill the missing values based on a linter interpolation\n",
    "            y[idx_nan] = itp(idx_nan)\n",
    "            return y\n",
    "        catch\n",
    "            return y\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# short test\n",
    "x = [2.5,missing,3.8,missing,8.9]\n",
    "LinInterp(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The function `LiInterp` can now be applied on each time series, so we would have a rather trival mapping of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "  f_{\\{time\\}}^{\\{time}\\} : \\mathcal{C}(\\{lat, lon, time, var\\}) \\rightarrow \\mathcal{C}(\\{lat, lon, time, var\\}).\n",
    "\\end{equation}\n",
    "\n",
    "For operations of this kind, the best is to use the `mapslices` function. In the ESDL package, this function needs the input function, the cube handle, and an indication on which dimension we would apply it. The function can then infer that the output dimension here is also an axis of type `Time`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cube_fill_itp = mapslices(LinInterp, cube_fill, dims = \"Time\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "As we describe in the paper, we estimate the intrinsic dimensions from the raw, yet gapfilled, data cube (`cube_fill_itp`), but also based on spectrally decomposed data. The decomposition via discrete FFTs is an atomic operation of the following form (Eq. 12),\n",
    "\n",
    "\\begin{equation}\n",
    "  f_{\\{time\\}}^{\\{time, freq\\}} : \\mathcal{C}(\\{lat, lon, time, var\\}) \\rightarrow \\mathcal{C}(\\{lat, lon, time, var, freq\\}).\n",
    "\\end{equation}\n",
    "\n",
    "which can be done using a pre-implemented ESDL function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Zarr\n",
    "cube_decomp = filterTSFFT(cube_fill_itp, compressor=Zarr.BloscCompressor(clevel=1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Estimate intrinic dimension via PCA\n",
    "\n",
    "For estimating the intrinsic estimation via PCA from a multivariate time series we need essenntially two atomic functions. First, dimensionality reduction,\n",
    "\n",
    "\\begin{equation}\n",
    "     f_{\\{time, var\\}}^{\\{time, princomp \\}} : \\mathcal{C}(\\{time, var\\}) \\rightarrow \\mathcal{C}(\\{time, princomp\\})\n",
    "\\end{equation}\n",
    "\n",
    "And second estimating from the reduced space the number of dimensions that represent more variance than the threshold (for details see paper):\n",
    "\\begin{equation}\n",
    "     f_{\\{time, princomp\\}}^{\\{ \\}} : \\mathcal{C}(\\{time, var\\}) \\rightarrow \\mathcal{C}(\\{int dim\\})\n",
    "\\end{equation}\n",
    "\n",
    "However, we as both steps emerge from the same analysis it is more efficient to wrap these two steps in a single atomic functions which has the structure:\n",
    "\\begin{equation}\n",
    "     f_{\\{time, var\\}}^{\\{ \\}} : \\mathcal{C}(\\{time, var\\}) \\rightarrow \\mathcal{C}(\\{\\})\n",
    "\\end{equation}\n",
    "\n",
    "We can now apply this to the cube: The latter was the operation described in the paper (Eq. 11) as\n",
    "\n",
    "\\begin{equation}\n",
    "     f_{\\{time, var\\}}^{\\{ \\}} : \\mathcal{C}(\\{lat, lon, time, var\\}) \\rightarrow \\mathcal{C}(\\{lat, lon\\})\n",
    "\\end{equation}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Function sufficient_dimensions should be available on every core, i.e. @everywhere\n",
    "@everywhere begin\n",
    "\n",
    "    # packages needed on each core\n",
    "    using MultivariateStats, Statistics\n",
    "\n",
    "    function sufficient_dimensions(xin::AbstractArray, expl_var::Float64 = 0.95)\n",
    "\n",
    "        any(ismissing,xin) && return NaN\n",
    "        npoint, nvar = size(xin)\n",
    "        means = mean(xin, dims = 1)\n",
    "        stds  = std(xin,  dims = 1)\n",
    "        xin   = broadcast((y,m,s) -> s>0.0 ? (y-m)/s : one(y), xin, means, stds)\n",
    "        pca = fit(PCA, xin', pratio = 0.999, method = :svd)\n",
    "        return findfirst(cumsum(principalvars(pca)) / tprincipalvar(pca) .> expl_var)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We first apply the function `cube_decomp`to the standard data cube with the threshold of 95% of retained variance. as we see from the description of the atomic function above, we need as minimum input dimension `Time` and `Variable`. We call the output cube `cube_int_dim`, which efficiently is a map."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cube_int_dim = mapslices(sufficient_dimensions, cube_fill_itp, 0.95, dims = (\"Time\",\"Variable\"))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Saving intermediate results can save CPU later, not needed to guarantee reproducability tough"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "saveCube(cube_int_dim, \"../data/IntDim\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now we apply the same function\n",
    "\n",
    "\\begin{equation}\n",
    "    f_{\\{time, var\\}}^{\\{ \\}} : \\mathcal{C}(\\{time, var\\}) \\rightarrow \\mathcal{C}(\\{\\})\n",
    "\\end{equation}\n",
    "\n",
    "to the spectrally decomposed cube (Eq. 13):\n",
    "\n",
    "\\begin{equation}\n",
    "       f_{\\{time, var\\}}^{\\{\\}} : \\mathcal{C}(\\{lat, lon, time, var, freq\\})\\rightarrow \\mathcal{C}(\\{lat, lon, freq\\})\n",
    "\\end{equation}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cube_int_dim_dec = mapslices(sufficient_dimensions, cube_decomp, 0.95, dims = (\"Time\",\"Variable\"))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "saveCube(cube_int_dim_dec, \"../data/IntDimDec\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Visualizing results is not part of the ESDL package.\n",
    "Here we rely on PyPlot to use the neat `cartopy`pacakge"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ccrs = pyimport_conda(\"cartopy.crs\",\"cartopy\")\n",
    "feat = pyimport_conda(\"cartopy.feature\",\"cartopy\")\n",
    "\n",
    "# function to plot global map\n",
    "# not generic - for this application only\n",
    "function plot_robin(titulo, DAT, clbtitle)\n",
    "\n",
    "    # make new figure\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "\n",
    "    # set the projection\n",
    "    ax = plt.subplot(1, 1, 1, projection=ccrs.Robinson())\n",
    "\n",
    "    # add title\n",
    "    plt.title(titulo, fontsize=18)\n",
    "\n",
    "    # land and ocean backgrounds\n",
    "    ax.add_feature(feat.LAND,  color = [0.9, 0.9, 0.9])\n",
    "    ax.add_feature(feat.OCEAN, color = [0.85, 0.85, 0.85])\n",
    "    ax.coastlines(resolution = \"50m\", color = [0, 0, 0], lw = 0.5)\n",
    "\n",
    "    # show data\n",
    "    im = ax.imshow(reverse(DAT', dims = 1), transform = ccrs.PlateCarree(), cmap = cm, vmin = 1.5, vmax = 12.5)\n",
    "\n",
    "    # add colobar\n",
    "    clb = plt.colorbar(im,\n",
    "        pad = 0.05,\n",
    "        shrink = 0.7,\n",
    "        aspect = 30,\n",
    "        orientation = \"horizontal\",\n",
    "        drawedges = \"false\",\n",
    "        extend = \"both\",\n",
    "        ticks = 2:12)\n",
    "\n",
    "    clb.ax.set_title(clbtitle)\n",
    "    ##plt.show()\n",
    "\n",
    "    return fig\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We write a little loop to plot and save all subplots of Fig. 4. One key information we need is, in which order the frequencies are saved so we do"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cube_int_dim_dec"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "println(getAxis(\"Scale\", cube_int_dim_dec).values)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "scale_name = getAxis(\"Scale\", cube_int_dim_dec).values"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# corresponding colormap\n",
    "cm = ColorMap(get_cmap(\"magma_r\", 11))\n",
    "\n",
    "prelab = [\"a) \", \"b) \", \"c) \", \"d )\"]\n",
    "\n",
    "# overwrite the first name\n",
    "scale_name = [\"Original Data\", \"Long-term variability\", \"Seasonal variability\", \"Short-term variability\"]\n",
    "\n",
    "save_name = [\"Original\", \"Long\", \"Annual\", \"Fast\"]\n",
    "\n",
    "# go through the four subplots\n",
    "for i in 1:4\n",
    "\n",
    "    # array access to get the results out of the cube\n",
    "    if i == 1\n",
    "        # cube_int_dim has only dimensions lat, lon so\n",
    "        VAL = cube_int_dim[:, :]\n",
    "    else\n",
    "        # cube_int_dim_dec has dimensions lat, lon, freq so\n",
    "        VAL = cube_int_dim_dec[i, :, :]\n",
    "    end\n",
    "\n",
    "    # missings -> NaN for PyPlot\n",
    "    DAT = zeros(size(VAL))./0.0 ## matrix of NaN\n",
    "    idx = findall(!ismissing, VAL) ## index if real vals\n",
    "    DAT[idx] = VAL[idx] ## only insert these\n",
    "\n",
    "    name = prelab[i]*scale_name[i]\n",
    "    fig = plot_robin(prelab[i]*scale_name[i], DAT, \"Intrinsic DImensions\")\n",
    "\n",
    "    savefig(\"../figures/IntDim_\" * save_name[i] * \".pdf\",\n",
    "        orientation = \"landscape\",\n",
    "        bbox_inches = \"tight\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fig  = figure(\"histplot\", figsize = (10, 10))\n",
    "\n",
    "lableg = [\"a) Original Data\", \"b) Long-term variability\", \"c) Seasonal variability\", \"d) Short-term variability\"]\n",
    "\n",
    "lat_ax_vals = getAxis(LatAxis, cube_int_dim.axes).values\n",
    "lon_ax_vals = getAxis(LonAxis, cube_int_dim.axes).values\n",
    "weights = cosd.(repeat(lat_ax_vals, inner = length(lon_ax_vals)))\n",
    "\n",
    "for i = 1:4\n",
    "    ax   = subplot(410 + i)\n",
    "\n",
    "    # lon x lat\n",
    "    data = i == 1 ? cube_int_dim[:, :] : cube_int_dim_dec[i, :, :]\n",
    "\n",
    "    data = vec(data)\n",
    "    idx_use =  findall(x -> !ismissing(x) && !isnan(x), data)\n",
    "\n",
    "    data = data[idx_use]\n",
    "    latw  = weights[idx_use]\n",
    "\n",
    "    N, bins, patches = ax.hist(\n",
    "        data,\n",
    "        weights = latw,\n",
    "        density = true,\n",
    "        alpha = 1,\n",
    "        bins = 0.5:1:13,\n",
    "        color = \"r\",\n",
    "        linewidth = 0,\n",
    "        label = lableg[i]\n",
    "    )\n",
    "\n",
    "    for j in eachindex(patches)\n",
    "        patches[j].set_facecolor(cm(max(j-1), 1))\n",
    "    end\n",
    "\n",
    "    hist(\n",
    "        data,\n",
    "        weights = latw,\n",
    "        density = true,\n",
    "        alpha = 1,\n",
    "        color = [0,0,0,1],\n",
    "        bins = 0.5:1:13,\n",
    "        linewidth = 1,\n",
    "        histtype=\"step\"\n",
    "    )\n",
    "\n",
    "\n",
    "    ##legend(frameon= \"False\",fontsize = 12)\n",
    "    ax.set_ylim(0, 0.6)\n",
    "    ax.set_xlim(0.5, 12.5)\n",
    "    xticks(1:12)\n",
    "    text(0.02, 0.85, lableg[i],fontsize = 14, transform = ax.transAxes)\n",
    "\n",
    "    if i == 1\n",
    "        ylabel(\"Weighted Frequency\", fontsize = 14)\n",
    "    end\n",
    "end\n",
    "\n",
    "xlabel(\"Intrinsic dimension\", fontsize = 14)\n",
    "\n",
    "savefig(\"../figures/IntDim_Hist.pdf\", bbox_inches = \"tight\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0-rc3.0"
  },
  "kernelspec": {
   "name": "julia-1.2",
   "display_name": "Julia 1.2.0-rc3.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
